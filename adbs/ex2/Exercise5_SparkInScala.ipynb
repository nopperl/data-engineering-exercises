{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Spark in Scala _[4 points]_\n",
    "\n",
    "In this exercise you have to solve the tasks given below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Elementary RDD functions \n",
    "\n",
    "(Brushing up on the basics of functional programming Scala)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  You are given a list of the first 20 numbers of the Fibbonacci numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://c100.local:8088/proxy/application_1587827373944_0715\n",
       "SparkContext available as 'sc' (version = 2.4.0-cdh6.3.2, master = yarn, app id = application_1587827373944_0715)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "fibs20: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fibs20 = sc.parallelize(List( 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Produce a list that contains all multiples of 3 from the list 'fibs20':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[Int] = Array(3, 21, 144, 987)\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fibs20.filter(x => x % 3 == 0 && x > 0).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Compute the median value of the list 'fibs20':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "median: (values: org.apache.spark.rdd.RDD[Int])Double\n",
       "res1: Double = 72.0\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def median(values: org.apache.spark.rdd.RDD[Int]) = {\n",
    "    val length = values.count()\n",
    "    val medianIndex = (math ceil length / 2.0).toInt\n",
    "    if (length % 2 == 1) {\n",
    "        values.collect()(medianIndex)\n",
    "    } else {\n",
    "        val middles = values.collect().slice(medianIndex, medianIndex + 2)\n",
    "        (middles(0) + middles(1)) / 2.0\n",
    "    }\n",
    "}\n",
    "\n",
    "median(fibs20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Produce a for the list 'fibs20' its standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stdev: (values: org.apache.spark.rdd.RDD[Int])Double\n",
       "res2: Double = 1055.812808929689\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stdev(values: org.apache.spark.rdd.RDD[Int]) = {\n",
    "    val length = values.count()\n",
    "    val sum = values.reduce((x,y) => x + y)\n",
    "    val avg = sum.toDouble / length\n",
    "    val diffs = values.map(x => math.pow((x - avg), 2))\n",
    "    val diffsum = diffs.reduce((x,y) => x + y)\n",
    "    math sqrt (diffsum / length)\n",
    "}\n",
    "\n",
    "stdev(fibs20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  You are given a random list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[3] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val words = sc.parallelize(List(\"pestilence\", \"virion\", \"quarantine\", \"vaccine\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Furthermore, we define a function that maps a word to its list of permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "permutate: (word: String)List[String]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def permutate (word:String) = word.permutations.toList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Produce a single list containing all permutations of elements from the list 'words':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[String] = Array(peeestilnc, peeestilcn, peeestinlc, peeestincl, peeesticln, peeesticnl, peeestlinc, peeestlicn, peeestlnic, peeestlnci, peeestlcin, peeestlcni, peeestnilc, peeestnicl, peeestnlic, peeestnlci, peeestncil, peeestncli, peeestciln, peeestcinl, peeestclin, peeestclni, peeestcnil, peeestcnli, peeesitlnc, peeesitlcn, peeesitnlc, peeesitncl, peeesitcln, peeesitcnl, peeesiltnc, peeesiltcn, peeesilntc, peeesilnct, peeesilctn, peeesilcnt, peeesintlc, peeesintcl, peeesinltc, peeesinlct, peeesinctl, peeesinclt, peeesictln, peeesictnl, peeesicltn, peeesiclnt, peeesicntl, peeesicnlt, peeesltinc, peeeslticn, peeesltnic, peeesltnci, peeesltcin, peeesltcni, peeeslitnc, peeeslitcn, peeeslintc, peeeslinct, peeeslictn, peeeslicnt, peeeslntic, peeeslntci, peeeslnitc, peeeslnict, p..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.flatMap(permutate).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) From SQL to Dataframe (and back again)\n",
    "\n",
    "#### Find for each of the Spark SQL queries an equivalent one that only uses the Dataframe API (or vice versa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "octDF: org.apache.spark.sql.DataFrame = [event_time: string, event_type: string ... 7 more fields]\n",
       "novDF: org.apache.spark.sql.DataFrame = [event_time: string, event_type: string ... 7 more fields]\n",
       "octRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[26] at rdd at <console>:29\n",
       "novRDD: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[30] at rdd at <console>:30\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val octDF = spark.read.options(Map(\"header\"->\"true\")).format(\"csv\").load(\"/home/adbs20/shared/ecommerce/2019-Oct.csv\")\n",
    "val novDF = spark.read.options(Map(\"header\"->\"true\")).format(\"csv\").load(\"/home/adbs20/shared/ecommerce/2019-Nov.csv\")\n",
    "octDF.createOrReplaceTempView(\"oct\")\n",
    "novDF.createOrReplaceTempView(\"nov\")\n",
    "// Create RDD view into dataset\n",
    "val octRDD = octDF.rdd\n",
    "val novRDD = novDF.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- category_id: string (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "octDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 1: Transform the given Spark SQL query into the Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 5282775|\n",
      "+--------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[count(1)])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "      +- *(1) Project\n",
      "         +- *(1) Filter (isnotnull(brand#15) && (brand#15 = samsung))\n",
      "            +- *(1) FileScan csv [brand#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/home/adbs20/shared/ecommerce/2019-Oct.csv], PartitionFilters: [], PushedFilters: [IsNotNull(brand), EqualTo(brand,samsung)], ReadSchema: struct<brand:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query1: org.apache.spark.sql.DataFrame = [count(1): bigint]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query1 = spark.sql(\"SELECT COUNT(*) FROM oct WHERE brand='samsung'\")\n",
    "query1.show() // 'false' turns of truncation of row entries\n",
    "query1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 5282775|\n",
      "+--------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[count(1)])\n",
      "+- Exchange SinglePartition\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_count(1)])\n",
      "      +- *(1) Project\n",
      "         +- *(1) Filter (isnotnull(brand#15) && (brand#15 = samsung))\n",
      "            +- *(1) FileScan csv [brand#15] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/home/adbs20/shared/ecommerce/2019-Oct.csv], PartitionFilters: [], PushedFilters: [IsNotNull(brand), EqualTo(brand,samsung)], ReadSchema: struct<brand:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query11: org.apache.spark.sql.DataFrame = [count(1): bigint]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query11 = octDF.where($\"brand\" === \"samsung\").agg(count(\"*\"))\n",
    "query11.show()\n",
    "query11.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 2: Transform the given Dataframe API query into Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+------+\n",
      "|brand                |count |\n",
      "+---------------------+------+\n",
      "|houseofseasons       |1318  |\n",
      "|edifier              |3565  |\n",
      "|yokohama             |128697|\n",
      "|tuffoni              |3617  |\n",
      "|tmnt                 |574   |\n",
      "|serebro              |7105  |\n",
      "|welss                |4929  |\n",
      "|tega                 |2317  |\n",
      "|vortex               |55    |\n",
      "|dvin                 |76    |\n",
      "|blaster              |122   |\n",
      "|crest                |325   |\n",
      "|receptybabuskiagafi  |22    |\n",
      "|lykan                |20    |\n",
      "|muimui               |5     |\n",
      "|riminibosco          |5     |\n",
      "|maximumpowernutrition|2     |\n",
      "|keenway              |2368  |\n",
      "|rosato               |698   |\n",
      "|marley               |1063  |\n",
      "+---------------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[brand#43], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(brand#43, 200)\n",
      "   +- *(1) HashAggregate(keys=[brand#43], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [brand#43] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/home/adbs20/shared/ecommerce/2019-Nov.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<brand:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query2: org.apache.spark.sql.DataFrame = [brand: string, count: bigint]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query2 = novDF.groupBy(\"brand\").count()\n",
    "query2.show(false) // 'false' turns of truncation of row entries\n",
    "query2.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------+\n",
      "|brand                |count(1)|\n",
      "+---------------------+--------+\n",
      "|yokohama             |128697  |\n",
      "|welss                |4929    |\n",
      "|serebro              |7105    |\n",
      "|edifier              |3565    |\n",
      "|tega                 |2317    |\n",
      "|tuffoni              |3617    |\n",
      "|crest                |325     |\n",
      "|tmnt                 |574     |\n",
      "|vortex               |55      |\n",
      "|houseofseasons       |1318    |\n",
      "|dvin                 |76      |\n",
      "|muimui               |5       |\n",
      "|blaster              |122     |\n",
      "|riminibosco          |5       |\n",
      "|receptybabuskiagafi  |22      |\n",
      "|maximumpowernutrition|2       |\n",
      "|lykan                |20      |\n",
      "|sonel                |11501   |\n",
      "|nutricia             |2496    |\n",
      "|keenway              |2368    |\n",
      "+---------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[brand#43], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(brand#43, 200)\n",
      "   +- *(1) HashAggregate(keys=[brand#43], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [brand#43] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/home/adbs20/shared/ecommerce/2019-Nov.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<brand:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query22: org.apache.spark.sql.DataFrame = [brand: string, count(1): bigint]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query22 = spark.sql(\"SELECT brand, COUNT(*) FROM nov GROUP BY brand\")\n",
    "query22.show(false)\n",
    "query22.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 3: Transform the given Spark SQL query into the Dataframe API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|user_id  |brand   |\n",
      "+---------+--------+\n",
      "|434170823|lucente |\n",
      "|441522689|oppo    |\n",
      "|461023190|fender  |\n",
      "|470193237|huawei  |\n",
      "|490354170|viatti  |\n",
      "|512369688|topface |\n",
      "|512388342|acer    |\n",
      "|512389344|samsung |\n",
      "|512392219|berghoff|\n",
      "|512395692|xiaomi  |\n",
      "|512397473|apple   |\n",
      "|512405720|jbl     |\n",
      "|512412345|indesit |\n",
      "|512420187|philips |\n",
      "|512420588|apple   |\n",
      "|512425298|xiaomi  |\n",
      "|512429071|dongma  |\n",
      "|512432852|apple   |\n",
      "|512442599|xiaomi  |\n",
      "|512444057|apple   |\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(7) Project [user_id#17, brand#15]\n",
      "+- *(7) Filter (count(1)#188L >= if (isnull(alwaysTrue#191)) 0 else count(brand)#186L)\n",
      "   +- SortMergeJoin [user_id#17], [user_id#17#190], LeftOuter\n",
      "      :- *(3) Sort [user_id#17 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(user_id#17, 200)\n",
      "      :     +- *(2) HashAggregate(keys=[user_id#17, brand#15], functions=[count(1)])\n",
      "      :        +- Exchange hashpartitioning(user_id#17, brand#15, 200)\n",
      "      :           +- *(1) HashAggregate(keys=[user_id#17, brand#15], functions=[partial_count(1)])\n",
      "      :              +- *(1) Project [brand#15, user_id#17]\n",
      "      :                 +- *(1) Filter ((isnotnull(event_type#11) && (event_type#11 = purchase)) && isnotnull(brand#15))\n",
      "      :                    +- *(1) FileScan csv [event_type#11,brand#15,user_id#17] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/home/adbs20/shared/ecommerce/2019-Oct.csv], PartitionFilters: [], PushedFilters: [IsNotNull(event_type), EqualTo(event_type,purchase), IsNotNull(brand)], ReadSchema: struct<event_type:string,brand:string,user_id:string>\n",
      "      +- *(6) Sort [user_id#17#190 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(user_id#17#190, 200)\n",
      "            +- *(5) HashAggregate(keys=[user_id#17], functions=[count(brand#15)])\n",
      "               +- Exchange hashpartitioning(user_id#17, 200)\n",
      "                  +- *(4) HashAggregate(keys=[user_id#17], functions=[partial_count(brand#15)])\n",
      "                     +- *(4) Project [brand#15, user_id#17]\n",
      "                        +- *(4) Filter ((isnotnull(event_type#11) && (event_type#11 = purchase)) && isnotnull(user_id#17))\n",
      "                           +- *(4) FileScan csv [event_type#11,brand#15,user_id#17] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/home/adbs20/shared/ecommerce/2019-Oct.csv], PartitionFilters: [], PushedFilters: [IsNotNull(event_type), EqualTo(event_type,purchase), IsNotNull(user_id)], ReadSchema: struct<event_type:string,brand:string,user_id:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query3: org.apache.spark.sql.DataFrame = [user_id: string, brand: string]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query3 = spark.sql(\"SELECT user_id, brand from oct where event_type ='purchase' AND brand IS NOT NULL GROUP BY user_id,brand HAVING COUNT(*) >= (SELECT COUNT(brand) from oct as oct2 where event_type ='purchase' AND oct2.user_id = oct.user_id GROUP BY user_id)  \")\n",
    "query3.show(false) // 'false' turns of truncation of row entries\n",
    "query3.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|user_id  |brand   |\n",
      "+---------+--------+\n",
      "|434170823|lucente |\n",
      "|441522689|oppo    |\n",
      "|461023190|fender  |\n",
      "|470193237|huawei  |\n",
      "|490354170|viatti  |\n",
      "|512369688|topface |\n",
      "|512388342|acer    |\n",
      "|512389344|samsung |\n",
      "|512392219|berghoff|\n",
      "|512395692|xiaomi  |\n",
      "|512397473|apple   |\n",
      "|512405720|jbl     |\n",
      "|512412345|indesit |\n",
      "|512420187|philips |\n",
      "|512420588|apple   |\n",
      "|512425298|xiaomi  |\n",
      "|512429071|dongma  |\n",
      "|512432852|apple   |\n",
      "|512442599|xiaomi  |\n",
      "|512444057|apple   |\n",
      "+---------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(6) Project [user_id#17, brand#15]\n",
      "+- *(6) SortMergeJoin [user_id_1#210], [user_id#17], Inner, (count#217L >= count2#207L)\n",
      "   :- *(3) Sort [user_id_1#210 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(user_id_1#210, 200)\n",
      "   :     +- *(2) HashAggregate(keys=[user_id_1#210, brand#15], functions=[count(1)])\n",
      "   :        +- Exchange hashpartitioning(user_id_1#210, brand#15, 200)\n",
      "   :           +- *(1) HashAggregate(keys=[user_id_1#210, brand#15], functions=[partial_count(1)])\n",
      "   :              +- *(1) Project [user_id#17 AS user_id_1#210, brand#15]\n",
      "   :                 +- *(1) Filter (((isnotnull(event_type#11) && (event_type#11 = purchase)) && isnotnull(brand#15)) && isnotnull(user_id#17))\n",
      "   :                    +- *(1) FileScan csv [event_type#11,brand#15,user_id#17] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/home/adbs20/shared/ecommerce/2019-Oct.csv], PartitionFilters: [], PushedFilters: [IsNotNull(event_type), EqualTo(event_type,purchase), IsNotNull(brand), IsNotNull(user_id)], ReadSchema: struct<event_type:string,brand:string,user_id:string>\n",
      "   +- *(5) Sort [user_id#17 ASC NULLS FIRST], false, 0\n",
      "      +- *(5) HashAggregate(keys=[user_id#17], functions=[count(brand#15)])\n",
      "         +- Exchange hashpartitioning(user_id#17, 200)\n",
      "            +- *(4) HashAggregate(keys=[user_id#17], functions=[partial_count(brand#15)])\n",
      "               +- *(4) Project [brand#15, user_id#17]\n",
      "                  +- *(4) Filter ((isnotnull(event_type#11) && (event_type#11 = purchase)) && isnotnull(user_id#17))\n",
      "                     +- *(4) FileScan csv [event_type#11,brand#15,user_id#17] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/home/adbs20/shared/ecommerce/2019-Oct.csv], PartitionFilters: [], PushedFilters: [IsNotNull(event_type), EqualTo(event_type,purchase), IsNotNull(user_id)], ReadSchema: struct<event_type:string,brand:string,user_id:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "subquery32: org.apache.spark.sql.DataFrame = [user_id: string, count2: bigint]\n",
       "query32: org.apache.spark.sql.DataFrame = [user_id: string, brand: string]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val subquery32 = octDF.where(\"event_type ='purchase'\").groupBy(\"user_id\").agg(count(\"brand\").alias(\"count2\"))\n",
    "val query32 = octDF.select($\"user_id\".alias(\"user_id_1\"), $\"brand\").where(\"event_type = 'purchase' and brand is not null\")\n",
    ".groupBy(\"user_id_1\", \"brand\").agg(count(\"*\").alias(\"count\"))\n",
    ".join(subquery32, $\"user_id_1\" === $\"user_id\")\n",
    ".filter($\"count\" >= $\"count2\")\n",
    ".select(\"user_id\", \"brand\")\n",
    "query32.show(false)\n",
    "query32.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query 4: Transform the given Dataframe API query into Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|category_id        |avgPrice          |\n",
      "+-------------------+------------------+\n",
      "|2053013557544485393|563.5393907361115 |\n",
      "|2053013560497275585|531.4014232236495 |\n",
      "|2053013558920217191|452.44963834043574|\n",
      "|2053013566503518219|432.4151428571424 |\n",
      "|2134905019189691101|431.7818269851515 |\n",
      "|2160912607672795565|420.6898284273414 |\n",
      "|2053013560555995845|415.17758445401313|\n",
      "|2053013561092866779|408.38360056087237|\n",
      "|2053013560463721151|392.2455597355071 |\n",
      "|2053013561059312345|382.69055976121973|\n",
      "|2116907525201723525|375.40895114020424|\n",
      "|2053013564523807647|374.87466637575403|\n",
      "|2160912655915679819|372.72334449178317|\n",
      "|2053013552695869677|372.65947867882153|\n",
      "|2053013555724157349|370.69754656974106|\n",
      "|2053013557645148695|363.24151916201055|\n",
      "|2105319817211805903|355.39508981216244|\n",
      "|2053013560530830019|355.10748622163754|\n",
      "|2109094148110811752|350.7298993914582 |\n",
      "|2053013559406756479|347.27060201997256|\n",
      "+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(7) Sort [avgPrice#329 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(avgPrice#329 DESC NULLS LAST, 200)\n",
      "   +- *(6) HashAggregate(keys=[category_id#13], functions=[avg(cast(price#44 as double))])\n",
      "      +- Exchange hashpartitioning(category_id#13, 200)\n",
      "         +- *(5) HashAggregate(keys=[category_id#13], functions=[partial_avg(cast(price#44 as double))])\n",
      "            +- *(5) Project [category_id#13, price#44]\n",
      "               +- *(5) SortMergeJoin [user_id#17], [user_id#45], Inner\n",
      "                  :- *(2) Sort [user_id#17 ASC NULLS FIRST], false, 0\n",
      "                  :  +- Exchange hashpartitioning(user_id#17, 200)\n",
      "                  :     +- *(1) Project [category_id#13, user_id#17]\n",
      "                  :        +- *(1) Filter ((isnotnull(event_type#11) && (event_type#11 = view)) && isnotnull(user_id#17))\n",
      "                  :           +- *(1) FileScan csv [event_type#11,category_id#13,user_id#17] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/home/adbs20/shared/ecommerce/2019-Oct.csv], PartitionFilters: [], PushedFilters: [IsNotNull(event_type), EqualTo(event_type,view), IsNotNull(user_id)], ReadSchema: struct<event_type:string,category_id:string,user_id:string>\n",
      "                  +- *(4) Sort [user_id#45 ASC NULLS FIRST], false, 0\n",
      "                     +- Exchange hashpartitioning(user_id#45, 200)\n",
      "                        +- *(3) Project [price#44, user_id#45]\n",
      "                           +- *(3) Filter isnotnull(user_id#45)\n",
      "                              +- *(3) FileScan csv [price#44,user_id#45] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/home/adbs20/shared/ecommerce/2019-Nov.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<price:string,user_id:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query4: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [category_id: string, avgPrice: double]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query4 = octDF\n",
    "      .join(novDF, octDF(\"user_id\") === novDF(\"user_id\"))\n",
    "      .filter(octDF(\"event_type\") === \"view\" )\n",
    "      .groupBy(octDF(\"category_id\"))\n",
    "      .agg(avg(novDF(\"price\")).as(\"avgPrice\"))\n",
    "      .orderBy(desc(\"avgPrice\"))\n",
    "query4.show(false) // 'false' turns of truncation of row entries\n",
    "query4.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|category_id        |avgPrice          |\n",
      "+-------------------+------------------+\n",
      "|2053013557544485393|563.5393907361117 |\n",
      "|2053013560497275585|531.4014232236497 |\n",
      "|2053013558920217191|452.4496383404357 |\n",
      "|2053013566503518219|432.41514285714237|\n",
      "|2134905019189691101|431.7818269851517 |\n",
      "|2160912607672795565|420.6898284273414 |\n",
      "|2053013560555995845|415.17758445401296|\n",
      "|2053013561092866779|408.3836005608722 |\n",
      "|2053013560463721151|392.2455597355071 |\n",
      "|2053013561059312345|382.6905597612197 |\n",
      "|2116907525201723525|375.4089511402043 |\n",
      "|2053013564523807647|374.874666375754  |\n",
      "|2160912655915679819|372.72334449178317|\n",
      "|2053013552695869677|372.65947867882153|\n",
      "|2053013555724157349|370.69754656974135|\n",
      "|2053013557645148695|363.2415191620105 |\n",
      "|2105319817211805903|355.3950898121623 |\n",
      "|2053013560530830019|355.1074862216376 |\n",
      "|2109094148110811752|350.7298993914584 |\n",
      "|2053013559406756479|347.27060201997267|\n",
      "+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "*(7) Sort [avgPrice#349 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(avgPrice#349 DESC NULLS LAST, 200)\n",
      "   +- *(6) HashAggregate(keys=[category_id#13], functions=[avg(cast(price#44 as double))])\n",
      "      +- Exchange hashpartitioning(category_id#13, 200)\n",
      "         +- *(5) HashAggregate(keys=[category_id#13], functions=[partial_avg(cast(price#44 as double))])\n",
      "            +- *(5) Project [category_id#13, price#44]\n",
      "               +- *(5) SortMergeJoin [user_id#17], [user_id#45], Inner\n",
      "                  :- *(2) Sort [user_id#17 ASC NULLS FIRST], false, 0\n",
      "                  :  +- Exchange hashpartitioning(user_id#17, 200)\n",
      "                  :     +- *(1) Project [category_id#13, user_id#17]\n",
      "                  :        +- *(1) Filter ((isnotnull(event_type#11) && (event_type#11 = view)) && isnotnull(user_id#17))\n",
      "                  :           +- *(1) FileScan csv [event_type#11,category_id#13,user_id#17] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/home/adbs20/shared/ecommerce/2019-Oct.csv], PartitionFilters: [], PushedFilters: [IsNotNull(event_type), EqualTo(event_type,view), IsNotNull(user_id)], ReadSchema: struct<event_type:string,category_id:string,user_id:string>\n",
      "                  +- *(4) Sort [user_id#45 ASC NULLS FIRST], false, 0\n",
      "                     +- Exchange hashpartitioning(user_id#45, 200)\n",
      "                        +- *(3) Project [price#44, user_id#45]\n",
      "                           +- *(3) Filter isnotnull(user_id#45)\n",
      "                              +- *(3) FileScan csv [price#44,user_id#45] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://nameservice1/home/adbs20/shared/ecommerce/2019-Nov.csv], PartitionFilters: [], PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<price:string,user_id:string>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query42: org.apache.spark.sql.DataFrame = [category_id: string, avgPrice: double]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val query42 = spark.sql(\"SELECT o.category_id, AVG(n.price) as avgPrice FROM oct o, nov n WHERE o.event_type = 'view' AND o.user_id = n.user_id GROUP BY o.category_id ORDER BY avgPrice DESC\")\n",
    "query42.show(false)\n",
    "query42.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For All Queries Above: \n",
    "#### Analyze the plans (.explain() ) and compare performance (using the Internal Spark Web UI). Try to reason about any major differences in the logical plans (if there are any)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Wide and Narrow Dependencies\n",
    "\n",
    "#### Look at the Dataframe queries given as part of b) or for which you wrote the Dataframe version.\n",
    "\n",
    "#### Use the Internal Spark Web UI to analyse the dependencies and stages of the queries, and try to determine which commands on which Dataframes are executed as wide dependencies and which as narrow dependencies. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (spylon-kernel)",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
