{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (MapReduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting favbrand.py\n"
     ]
    }
   ],
   "source": [
    "%%file favbrand.py\n",
    "from mrjob.job import MRJob\n",
    "from collections import Counter\n",
    "from mr3px.csvprotocol import CsvProtocol\n",
    "\n",
    "class MRFavouriteBrand(MRJob):\n",
    "    OUTPUT_PROTOCOL = CsvProtocol\n",
    "    def mapper(self, _, line):\n",
    "        if line.startswith(\"event_time\"):\n",
    "            return\n",
    "        cells = line.split(\",\")\n",
    "        event_type = cells[1]\n",
    "        product_id = cells[2]\n",
    "        brand = cells[5]\n",
    "        user_id = cells[7]\n",
    "        if event_type != \"purchase\" or not brand or not product_id:\n",
    "            return\n",
    "        yield user_id, brand\n",
    "\n",
    "    def reducer(self, user_id, brands):\n",
    "        brand_counts = Counter(brands)\n",
    "        fav_brand = brand_counts.most_common(1)[0][0]\n",
    "        yield None, (user_id, fav_brand)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRFavouriteBrand.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 1...\n",
      "Creating temp directory /tmp/favbrand.e0test.20200421.160325.770096\n",
      "job output is in /tmp/favbrand.e0test.20200421.160325.770096/output\n",
      "Streaming final output from /tmp/favbrand.e0test.20200421.160325.770096/output...\n",
      "Removing temp directory /tmp/favbrand.e0test.20200421.160325.770096...\n"
     ]
    }
   ],
   "source": [
    "! python favbrand.py /home/adbs20/shared/ecommerce/2019-Oct-short.csv >> output-favbrand-short.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/ python favbrand.py -r hadoop hdfs:///home/adbs20/shared/ecommerce/2019-Oct.csv > output-favbrand.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting brandcount.py\n"
     ]
    }
   ],
   "source": [
    "%%file brandcount.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from mr3px.csvprotocol import CsvProtocol\n",
    "        \n",
    "class MRBrandCountCombined(MRJob):\n",
    "    OUTPUT_PROTOCOL = CsvProtocol\n",
    "    \n",
    "    def mapper_brand_users(self, _, line):\n",
    "        if line.startswith(\"event_time\"):\n",
    "            return\n",
    "        cells = line.split(\",\")\n",
    "        event_time = cells[0]\n",
    "        if not event_time:\n",
    "            return\n",
    "        try:\n",
    "            event_time = datetime.strptime(event_time.split()[0], \"%Y-%m-%d\")\n",
    "        except:\n",
    "            return\n",
    "        event_type = cells[1]\n",
    "        product_id = cells[2]\n",
    "        brand = cells[5]\n",
    "        user_id = cells[7]\n",
    "        if event_type != \"purchase\" or not brand or not product_id:\n",
    "            return\n",
    "        yield user_id,(event_time.month, brand)\n",
    "\n",
    "    def reducer_fav_brand(self, user_id, values):\n",
    "        values = list(values)\n",
    "        months = map(itemgetter(0), values)\n",
    "        if not (10 in months and 11 in months):\n",
    "            return\n",
    "        brands = map(itemgetter(1), values)\n",
    "        brand_counts = Counter(brands)\n",
    "        most_common_brands = brand_counts.most_common(2)\n",
    "        if len(most_common_brands) > 1 and\\\n",
    "            most_common_brands[0][1] == most_common_brands[1][1]:\n",
    "            return\n",
    "        fav_brand = most_common_brands[0][0]\n",
    "        yield user_id, fav_brand\n",
    "    \n",
    "    def inverse_mapper(self, key, value):\n",
    "        yield value, key\n",
    "    \n",
    "    def reducer_brand_count(self, brand, users):\n",
    "        yield None, (brand, len(list(users)))\n",
    "        \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_brand_users,\n",
    "                  reducer=self.reducer_fav_brand),\n",
    "            MRStep(mapper=self.inverse_mapper,\n",
    "                  reducer=self.reducer_brand_count)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRBrandCountCombined.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Running step 1 of 2...\n",
      "Creating temp directory /tmp/brandcount.e0test.20200421.154503.834139\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/brandcount.e0test.20200421.154503.834139/output\n",
      "Streaming final output from /tmp/brandcount.e0test.20200421.154503.834139/output...\n",
      "\"apple\",3\n",
      "\"samsung\",2\n",
      "Removing temp directory /tmp/brandcount.e0test.20200421.154503.834139...\n"
     ]
    }
   ],
   "source": [
    "! python brandcount.py /home/adbs20/shared/ecommerce/2019-Oct-short.csv /home/adbs20/shared/ecommerce/2019-Nov-short.csv >> output-brandcount-short.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a Hadoop job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/bin...\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 3.0.0\n",
      "Looking for Hadoop streaming jar in /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/...\n",
      "Found Hadoop streaming jar: /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Creating temp directory /tmp/favbrand.e0test.20200421.155801.981212\n",
      "Copying local files to hdfs:///user/e0test/tmp/mrjob/favbrand.e0test.20200421.155801.981212/files/...\n",
      "Running step 1 of 1...\n",
      "  WARNING: Use \"yarn jar\" to launch YARN applications.\n",
      "  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "  packageJobJar: [] [/opt/cloudera/parcels/CDH-6.3.2-1.cdh6.3.2.p0.1605554/jars/hadoop-streaming-3.0.0-cdh6.3.2.jar] /tmp/streamjob2757712408007014417.jar tmpDir=null\n",
      "  Connecting to ResourceManager at c100.local/10.7.0.100:8032\n",
      "  Connecting to ResourceManager at c100.local/10.7.0.100:8032\n",
      "  Disabling Erasure Coding for path: /user/e0test/.staging/job_1586332778980_11235\n",
      "  Total input files to process : 1\n",
      "  number of splits:43\n",
      "  yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\n",
      "  Submitting tokens for job: job_1586332778980_11235\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1586332778980_11235\n",
      "  The url to track the job: http://c100.local:8088/proxy/application_1586332778980_11235/\n",
      "  Running job: job_1586332778980_11235\n",
      "  Job job_1586332778980_11235 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 7% reduce 0%\n",
      "   map 12% reduce 0%\n",
      "   map 19% reduce 0%\n",
      "   map 40% reduce 0%\n",
      "   map 49% reduce 0%\n",
      "   map 79% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1586332778980_11235 completed successfully\n",
      "  Output directory: hdfs:///user/e0test/tmp/mrjob/favbrand.e0test.20200421.155801.981212/output\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=5671365367\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=7188415\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4596939\n",
      "\t\tFILE: Number of bytes written=20743943\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=5671370226\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=7188415\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=134\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=12\n",
      "\t\tLaunched map tasks=43\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=31\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=2876794880\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=73000960\n",
      "\t\tTotal time spent by all map tasks (ms)=561874\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=29217448\n",
      "\t\tTotal time spent by all reduce tasks (ms)=14258\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=741416\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=561874\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=14258\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=305270\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=38856\n",
      "\t\tInput split bytes=4859\n",
      "\t\tMap input records=42448765\n",
      "\t\tMap output bytes=14364382\n",
      "\t\tMap output materialized bytes=6166374\n",
      "\t\tMap output records=684635\n",
      "\t\tMerged Map outputs=43\n",
      "\t\tPeak Map Physical memory (bytes)=1237487616\n",
      "\t\tPeak Map Virtual memory (bytes)=6298157056\n",
      "\t\tPeak Reduce Physical memory (bytes)=483467264\n",
      "\t\tPeak Reduce Virtual memory (bytes)=6303465472\n",
      "\t\tPhysical memory (bytes) snapshot=39320948736\n",
      "\t\tReduce input groups=326222\n",
      "\t\tReduce input records=684635\n",
      "\t\tReduce output records=326222\n",
      "\t\tReduce shuffle bytes=6166374\n",
      "\t\tShuffled Maps =43\n",
      "\t\tSpilled Records=1369270\n",
      "\t\tTotal committed heap usage (bytes)=99216785408\n",
      "\t\tVirtual memory (bytes) snapshot=276934656000\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/e0test/tmp/mrjob/favbrand.e0test.20200421.155801.981212/output\n",
      "Streaming final output from hdfs:///user/e0test/tmp/mrjob/favbrand.e0test.20200421.155801.981212/output...\n",
      "STDERR: 20/04/21 17:59:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Removing HDFS temp directory hdfs:///user/e0test/tmp/mrjob/favbrand.e0test.20200421.155801.981212...\n",
      "Removing temp directory /tmp/favbrand.e0test.20200421.155801.981212...\n"
     ]
    }
   ],
   "source": [
    "! HADOOP_HOME=/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/ python brandcount.py -r hadoop hdfs:///home/adbs20/shared/ecommerce/2019-Oct.csv hdfs:///home/adbs20/shared/ecommerce/2019-Nov.csv > output-brandcount.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
