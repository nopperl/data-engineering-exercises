{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\chi^2$ selection of most discriminative terms per category\n",
    "\n",
    "Some implementation details for the Spark version:\n",
    "*  Instead of the distributed cache functionality in Hadoop, all larger values are distributed to partitions using broadcast variables. This refers to the `stopwords.txt` file and the outputs of the intermediary overall term frequency and category frequency \"stages\"\n",
    "* Most functionality was implemented without `groupByKey` (except for the last merge job), this might be the reason why the Spark implementation is faster.\n",
    "* The input file is read using the Spark JSON parser and the resulting DataFrame is converted into an `RDD[Row]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://localhost:8088/proxy/application_1587827373944_3898\n",
       "SparkContext available as 'sc' (version = 2.4.0-cdh6.3.2, master = yarn, app id = application_1587827373944_3898)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import scala.io.Source\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.io.Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewsFile: String = hdfs:///scratch/amazon-reviews/full/reviews_devset.json\n",
       "outputPath: String = hdfs:///scratch/e0test/dic2/output_rdd\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reviewsFile = \"hdfs:///scratch/amazon-reviews/full/reviews_devset.json\"\n",
    "val outputPath = \"hdfs:///scratch/e0test/dic2/output_rdd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewsDF: org.apache.spark.sql.DataFrame = [asin: string, category: string ... 8 more fields]\n",
       "reviews: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[6] at rdd at <console>:28\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reviewsDF = spark.read.json(reviewsFile)\n",
    "val reviews = reviewsDF.select(\"category\", \"reviewText\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.Row = [Patio_Lawn_and_Garde,This was a gift for my other husband.  He's making us things from it all the time and we love the food.  Directions are simple, easy to read and interpret, and fun to make.  We all love different kinds of cuisine and Raichlen provides recipes from everywhere along the barbecue trail as he calls it. Get it and just open a page.  Have at it.  You'll love the food and it has provided us with an insight into the culture that produced it. It's all about broadening horizons.  Yum!!]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopwords: scala.collection.immutable.Set[String] = Set(serious, latterly, looks, particularly, used, down, regarding, entirely, it's, regardless, moreover, please, ourselves, able, that's, behind, for, despite, maybe, viz, further, corresponding, any, wherein, across, name, allows, there's, this, haven't, instead, in, ought, myself, have, your, off, once, i'll, are, is, his, oh, why, rd, knows, too, among, course, greetings, somewhat, everyone, seen, likely, said, try, already, soon, nobody, got, given, using, less, am, consider, hence, than, accordingly, isn't, four, didn't, anyhow, want, three, forth, whereby, himself, specify, yes, throughout, inasmuch, but, you're, whether, sure, below, co, best, plus, becomes, what, unto, different, would, although, elsewhere, causes, another, cer..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopwords = Source.fromFile(\"stopwords.txt\").getLines().toSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopwordsVar: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Set[String]] = Broadcast(4)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stopwordsVar = sc.broadcast(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "preprocess: (line: org.apache.spark.sql.Row, stopwords: org.apache.spark.broadcast.Broadcast[Set[String]])Set[(String, String)]\n",
       "preprocessedReviews: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[7] at flatMap at <console>:43\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess(line: org.apache.spark.sql.Row, stopwords: org.apache.spark.broadcast.Broadcast[Set[String]]): Set[(String, String)] = {\n",
    "    val delimiters = \"\"\"\\s|\\d|\\.|!|\\?|,|;|:|\\(|\\)|\\[|]|\\{|}|-|_|\"|`|~|#|&|\\*|%|\\$|\\|/\"\"\"\n",
    "    val category = line(0).asInstanceOf[String]\n",
    "    val text = line(1).asInstanceOf[String]\n",
    "    val unigrams = text.split(delimiters).toSet\n",
    "    val tokens = unigrams -- stopwords.value\n",
    "    for (token <- tokens) yield {\n",
    "        val tokenLower = token.toLowerCase()\n",
    "        (category, tokenLower)\n",
    "    }\n",
    "}\n",
    "\n",
    "val preprocessedReviews = reviews.flatMap(x => preprocess(x, stopwordsVar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: (String, String) = (Patio_Lawn_and_Garde,make)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessedReviews.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "swappedReviews: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[8] at map at <console>:27\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val swappedReviews = preprocessedReviews.map(pair => pair.swap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: (String, String) = (make,Patio_Lawn_and_Garde)\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swappedReviews.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "termFrequencies: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[9] at aggregateByKey at <console>:27\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val termFrequencies = swappedReviews.aggregateByKey(0)((n, v) => n + 1, (a, b) => a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: (String, Int) = (vecindad,1)\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termFrequencies.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categories: org.apache.spark.rdd.RDD[(Any, Int)] = MapPartitionsRDD[10] at map at <console>:27\n",
       "categoryFrequencies: org.apache.spark.rdd.RDD[(Any, Int)] = ShuffledRDD[11] at reduceByKey at <console>:28\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val categories = reviews.map(x => (x(0), 1))\n",
    "val categoryFrequencies = categories.reduceByKey((a, b) => a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: (Any, Int) = (Pet_Supplie,1235)\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoryFrequencies.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "termFrequenciesVar: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[String,Int]] = Broadcast(12)\n",
       "categoryFrequenciesVar: org.apache.spark.broadcast.Broadcast[scala.collection.immutable.Map[Any,Int]] = Broadcast(14)\n",
       "totalRecordsVar: org.apache.spark.broadcast.Broadcast[Long] = Broadcast(16)\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val termFrequenciesVar = sc.broadcast(termFrequencies.collect().toMap)\n",
    "val categoryFrequenciesVar = sc.broadcast(categoryFrequencies.collect().toMap)\n",
    "val totalRecordsVar = sc.broadcast(reviews.count()) // could also be implemented using a counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`categoryTermReduce` is the main function of the job. It is applied to each `(category, [term])` pair and is side joined with the term frequencies and category frequencies. Using this information, the $\\chi^2$ value is computed and a sorted `(category, [term])` pair is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoryTermReduce: (category: Any, terms: Iterable[Any], categoryFrequencies: org.apache.spark.broadcast.Broadcast[Map[Any,Int]], termFrequencies: org.apache.spark.broadcast.Broadcast[Map[String,Int]], totalRecords: org.apache.spark.broadcast.Broadcast[Long])(Any, Iterable[(String, Double)])\n",
       "result: org.apache.spark.rdd.RDD[(Any, Iterable[(String, Double)])] = MapPartitionsRDD[13] at map at <console>:58\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def categoryTermReduce(category: Any,\n",
    "                       terms: Iterable[Any],\n",
    "                       categoryFrequencies: org.apache.spark.broadcast.Broadcast[Map[Any,Int]],\n",
    "                       termFrequencies: org.apache.spark.broadcast.Broadcast[Map[String,Int]],\n",
    "                       totalRecords: org.apache.spark.broadcast.Broadcast[Long]): (Any, Iterable[(String, Double)]) = {\n",
    "    val termFrequenciesInCategory = terms.groupBy(identity).mapValues(_.size)\n",
    "    val x2map = for (x <- termFrequenciesInCategory) yield {\n",
    "        val term = x._1.toString\n",
    "        val A = x._2\n",
    "        val G = categoryFrequencies.value(category)\n",
    "        val E = termFrequencies.value(term)\n",
    "        val F = totalRecords.value - E\n",
    "        val C = G - A\n",
    "        val B = E - A\n",
    "        val D = F - C\n",
    "        val x2 = math.pow((A * D - B * C), 2) / ((A + B) * (A + C) * (B + D) * (C + D))\n",
    "        (term, x2)\n",
    "    }\n",
    "    \n",
    "    val x2top = x2map.toList.sortBy(x => -x._2).take(200)\n",
    "    (category, x2top)\n",
    "}\n",
    "\n",
    "val result = preprocessedReviews.groupByKey().map(x => categoryTermReduce(x._1, x._2, categoryFrequenciesVar, termFrequenciesVar, totalRecordsVar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a (term,$\\chi^2$) tuple for each category. This could now be further tranformed and formatted into a nicer string representation, but for now, it is simply dumped in a text file as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: (Any, Iterable[(String, Double)]) = (Pet_Supplie,List((dog,0.11808064730315776), (dogs,0.0693459289377741), (cat,0.05555264367426396), (cats,0.05181526766625477), (litter,0.021968767375091326), (pet,0.02176846068868463), (puppy,0.020849370279927015), (leash,0.018338556982611172), (collar,0.018183899560217088), (vet,0.018089186938466333), (treats,0.015990441639137484), (chew,0.013506403042664305), (fleas,0.011293069645448874), (food,0.010075133881935329), (lab,0.008638117840062135), (chewed,0.00850943133729101), (terrier,0.007847700838820342), (barking,0.007395087831878466), (tank,0.006430340680335897), (chewer,0.006376894461889176), (kitten,0.006003710080323079), (paw,0.005990132121567097), (dog's,0.005790685132380752), (crate,0.0057155866159485065), (aquarium,0.005715586615948506..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.saveAsTextFile(outputPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (spylon-kernel)",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
