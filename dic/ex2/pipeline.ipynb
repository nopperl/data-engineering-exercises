{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Text Classification\n",
    "\n",
    "This notebook contains the text classification pipeline for the reviews dataset. It can be submitted as a long-running job by converting it into a python script file:\n",
    "\n",
    "`jupyter nbconvert --to script pipeline.ipynb`\n",
    "\n",
    "Then, submit the job using:\n",
    "\n",
    "`spark-submit pipeline.py`\n",
    "\n",
    "Some implementation details:\n",
    "\n",
    "For efficiency, I try to get as much preprocessing as possible done before the hyperparameter search. But, since no information from the test/validation set should leak to train set, idf values and $\\chi^2$ feature selection is done on each set separately. Since the feature selection is longest stage in preprocessing, the efficiency gains are minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from pyspark import SparkContext, SQLContext\n",
    "from pyspark.ml import Pipeline, PipelineModel, Transformer\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import ChiSqSelector, CountVectorizer, IDF, Normalizer, OneHotEncoderEstimator, RegexTokenizer, StopWordsRemover, StringIndexer\n",
    "from pyspark.ml.param.shared import HasInputCol\n",
    "from pyspark.ml.tuning import TrainValidationSplit, TrainValidationSplitModel, ParamGridBuilder\n",
    "from pyspark.ml.util import MLReadable, MLWritable\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "sqlc = SQLContext(sc)\n",
    "reviews_file = \"hdfs:///user/pknees/amazon-reviews/full/reviews_devset.json\"\n",
    "selected_terms_file = \"output_ds.txt\"\n",
    "model_file = f\"dic2/model-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "results_file = f\"results-{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.txt\"\n",
    "seed = 42  # seed is used for the dataset splits and the decision tree for reproducibility reasons\n",
    "save_selected_terms = False\n",
    "classifier = \"decision_tree\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there seem to be resource contraints on the Jupyter runtime, only 1000 rows of the whole dataset are loaded. Change this by removing the `limit(1000)` call below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlc.read.json(reviews_file).select(\"category\", \"reviewText\").limit(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "The preprocessing pipeline leverages built-in Transformers and Estimators. Only for case folding a custom Transformer using a User Defined Function is needed. This is also the only stage which changes a column in-place, all other stages simply append to the data frame.\n",
    "\n",
    "The feature selection is implemented using ChiSqSelector over the term frequencies, in order to maintain comparability with the MapReduce/RDD jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaseFolder(Transformer, HasInputCol, MLReadable, MLWritable):\n",
    "    def __init__(self, inputCol):\n",
    "        super().__init__()\n",
    "        self.setInputCol(inputCol)\n",
    "        self.udf = UserDefinedFunction(lambda x: x.lower(), StringType())\n",
    "    \n",
    "    def _transform(self, df):\n",
    "        new_df = df.withColumn(self.getInputCol(), self.udf(df[self.getInputCol()]))\n",
    "        return new_df\n",
    "\n",
    "casefolder = CaseFolder(inputCol=\"reviewText\")\n",
    "tokenizer = RegexTokenizer(inputCol=\"reviewText\", outputCol=\"rawTerms\", pattern=\"\"\"\\s|\\d|\\.|!|\\?|,|;|:|\\(|\\)|\\[|]|\\{|}|-|_|\"|`|~|#|&|\\*|%|\\$|\\|/\"\"\")\n",
    "stopWordsRemover = StopWordsRemover(inputCol=\"rawTerms\", outputCol=\"filteredTerms\")\n",
    "categoryIndexer = StringIndexer(inputCol=\"category\", outputCol=\"categoryIndex\")\n",
    "tfTransformer = CountVectorizer(inputCol=\"filteredTerms\", outputCol=\"termFrequencies\")\n",
    "chisq = ChiSqSelector(numTopFeatures=4000, featuresCol=\"termFrequencies\", labelCol=\"categoryIndex\", outputCol=\"selectedTerms\")\n",
    "preprocessingPipeline = Pipeline(stages=[casefolder, tokenizer, stopWordsRemover, categoryIndexer, tfTransformer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproModel = preprocessingPipeline.fit(df)\n",
    "preproDF = preproModel.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing pipeline needs to be fit to the whole dataset such that the Estimator stages can be fitted on the dataset. This is the case for the CountVectorizer, which maintains a term->ix mapping.\n",
    "\n",
    "Since the feature selection using the ChiSqSelector is not part of the overall preprocessing (it is later applied to the dataset splits), to get an overall selection of terms the ChiSqSelector needs to be fitted explicitly below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_selected_terms:\n",
    "    chisq_model = chisq.fit(preproDF)\n",
    "    vocabulary = preproModel.stages[4].vocabulary\n",
    "    selectedTerms = [vocabulary[index] for index in chisq_model.selectedFeatures]\n",
    "    with open(selected_terms_file, \"w\") as file:\n",
    "        file.write(\", \".join(selectedTerms))\n",
    "    selectedTerms[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A train dataset (which is later split into a train and validation set) is used to fit the classifier, while the test dataset is used to evaluate the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = preproDF.randomSplit([.8, .2], seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proper pipeline contians stages for dataset split-specific preprocessing and the classifiers. If a binary classifier like LinearSVC is used, a OneVsRest strategy has to be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if classifier == \"svm\":\n",
    "    idf = IDF(inputCol=\"selectedTerms\", outputCol=\"tfidf\")\n",
    "    normalizer = Normalizer(p=2, inputCol=\"selectedTerms\", outputCol=\"features\")\n",
    "    svm = LinearSVC(featuresCol=\"features\", labelCol=\"categoryIndex\", predictionCol=\"prediction\")\n",
    "    ovr = OneVsRest(classifier=svm, featuresCol=\"features\", labelCol=\"categoryIndex\", predictionCol=\"prediction\")\n",
    "    pipeline = Pipeline(stages=[chisq, idf, normalizer, ovr])\n",
    "elif classifier == \"decision_tree\":\n",
    "    idf = IDF(inputCol=\"selectedTerms\", outputCol=\"tfidf\")\n",
    "    normalizer = Normalizer(p=2, inputCol=\"selectedTerms\", outputCol=\"features\")\n",
    "    decision_tree = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"categoryIndex\", predictionCol=\"prediction\", seed=seed)\n",
    "    pipeline = Pipeline(stages=[chisq, idf, normalizer, decision_tree])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter grid allows us to test multiple hyperparameter combinations of pipeline stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if classifier == \"svm\":\n",
    "    paramGrid = ParamGridBuilder()\\\n",
    "        .addGrid(chisq.numTopFeatures, [400, 4000])\\\n",
    "        .addGrid(svm.regParam, [1, .1, 0])\\\n",
    "        .addGrid(svm.maxIter, [100, 10])\\\n",
    "        .addGrid(svm.standardization, [True, False])\\\n",
    "        .build()\n",
    "elif classifier == \"decision_tree\":\n",
    "    paramGrid = ParamGridBuilder()\\\n",
    "        .addGrid(chisq.numTopFeatures, [400, 4000])\\\n",
    "        .addGrid(decision_tree.maxDepth, [2, 4, 9, 15])\\\n",
    "        .addGrid(decision_tree.impurity, [\"entropy\", \"gini\"])\\\n",
    "        .addGrid(decision_tree.minInstancesPerNode, [1, 100])\\\n",
    "        .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using TrainValidationSplit, the pipeline is fitted on the train dataset and validated on the validation dataset for each parameter combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"categoryIndex\", metricName=\"f1\")\n",
    "validation = TrainValidationSplit(estimator=pipeline,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  evaluator=evaluator,\n",
    "                                  trainRatio=0.8,\n",
    "                                  seed=seed)\n",
    "\n",
    "validationModel = validation.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validationModel.bestModel.save(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = PipelineModel.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_file, \"w\") as file:\n",
    "    file.write(\",\".join(map(str, validationModel.validationMetrics)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best hyperparameter combination is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.explainParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to further evaluate the best model, the F1 score on the test/holdout set is computed. It would also be possible to fit a new model using the best hyperparameter combination on the whole train dataset (train + validation) and then evaluate this model with the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = best_model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test dataset: 0.6802603841579735\n"
     ]
    }
   ],
   "source": [
    "overall_evaluation = evaluator.evaluate(predictions_test)\n",
    "print(f\"F1 score on test dataset: {overall_evaluation}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "python",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
